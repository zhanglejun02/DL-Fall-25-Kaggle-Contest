{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":117027,"databundleVersionId":13972231,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13573951,"sourceType":"datasetVersion","datasetId":8623033}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n!pip -q install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n!pip -q install datasets transformers trl peft bitsandbytes accelerate scikit-learn pandas\n\n\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nimport random, numpy as np, gc, hashlib\nimport torch\nfrom typing import List\nfrom datasets import load_dataset\nfrom unsloth import FastLanguageModel\nfrom peft import PeftModel  \nfrom transformers import LogitsProcessor, TrainingArguments #  LogitsProcessor\nfrom sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\nimport pandas as pd\n\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n\nprint(\"finish\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T23:36:25.531886Z","iopub.execute_input":"2025-11-01T23:36:25.532139Z","iopub.status.idle":"2025-11-01T23:39:32.470083Z","shell.execute_reply.started":"2025-11-01T23:36:25.532110Z","shell.execute_reply":"2025-11-01T23:39:32.469173Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m273.6/273.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m755.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m260.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-01 23:38:42.894260: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762040323.256026      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762040323.362750      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\nfinish\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nADAPTER_PATH = \"/kaggle/input/my-trained-adapters-v2\"#/kaggle/input/my-trained-adapters-v2  /kaggle/input/my-trained-adapters-v2\n\n\nMAX_LEN_GEN  = 640  \ndtype = None\nload_in_4bit = True\n\n# check device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")\nprint(f\"add from: {ADAPTER_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T23:39:32.471959Z","iopub.execute_input":"2025-11-01T23:39:32.472192Z","iopub.status.idle":"2025-11-01T23:39:32.476957Z","shell.execute_reply.started":"2025-11-01T23:39:32.472175Z","shell.execute_reply":"2025-11-01T23:39:32.476155Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nadd from: /kaggle/input/my-trained-adapters-v2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\n\nprint(\"from Hugging Face  'unsloth/Meta-Llama-3.1-8B' (4-bit)...\")\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name   = \"unsloth/Meta-Llama-3.1-8B\", \n    max_seq_length = MAX_LEN_GEN,\n    dtype        = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\nprint(\"\\nLora...\")\ntry:\n    \n    model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n    print(\"adapter readyï¼\")\nexcept Exception as e:\n    print(f\"\\n{'='*50}\\n '{ADAPTER_PATH}' rightï¼Ÿ\\n not: {e}\\n{'='*50}\")\n    \nprint(\"\\n (Unsloth)...\")\n\nFastLanguageModel.for_inference(model) \nprint(\"model ready\")\n\n# ---  Tokenizer ---\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"   \ntokenizer.truncation_side = \"left\" \ntry:\n    model.config.pad_token_id = tokenizer.pad_token_id\nexcept Exception:\n    pass\n\n# ---  Token IDs ---\ntrue_ids  = tokenizer.encode(\" True\", add_special_tokens=False)\nfalse_ids = tokenizer.encode(\" False\", add_special_tokens=False)\nallowed_first_tokens = list({ true_ids[0], false_ids[0] })\nprint(f\"True IDs: {true_ids}, False IDs: {false_ids}\")\n\ngc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T23:39:32.477636Z","iopub.execute_input":"2025-11-01T23:39:32.477896Z","iopub.status.idle":"2025-11-01T23:40:20.396109Z","shell.execute_reply.started":"2025-11-01T23:39:32.477874Z","shell.execute_reply":"2025-11-01T23:40:20.395306Z"}},"outputs":[{"name":"stdout","text":"from Hugging Face  'unsloth/Meta-Llama-3.1-8B' (4-bit)...\n==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.57.1.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"438ae0f537924b32b84567151fbd6b20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aabe6b8247594b1f849f416c2c813064"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeecf8c92bb7481b8f2342a3a766885d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ff3642d6e574cbd8dad6069e9c4aa71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"114d67726c55418a99744279871f31af"}},"metadata":{}},{"name":"stdout","text":"\nLora...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['target_parameters'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"adapter readyï¼\n\n (Unsloth)...\nmodel ready\nTrue IDs: [3082], False IDs: [3641]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\nprint(\"from Hugging Face ..\")\n\nfull = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"train\")\n\ndef qhash(q):\n    return int(hashlib.md5(q.encode(\"utf-8\")).hexdigest(), 16) % (10**9)\n\ngroups = [qhash(q) for q in full[\"question\"]]\nrng = np.random.default_rng(SEED)\nunique_groups = np.array(sorted(set(groups)))\nrng.shuffle(unique_groups)\ncut = int(len(unique_groups) * 0.9)\ntrain_group_set = set(unique_groups[:cut])\nvalid_idx = [i for i, g in enumerate(groups) if g not in train_group_set]\nvalid_ds = full.select(valid_idx)\ny_true_valid = [int(x) for x in valid_ds[\"is_correct\"]] \n\ntest_ds = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n\nPROMPTS = [\n    \"Q:\\n{}\\nA:\\n{}\\nOutput:\\n\",\n    \"Question:\\n{}\\nStudent Answer:\\n{}\\nOutput:\\n\",\n    \"Problem:\\n{}\\nSolution:\\n{}\\nOutput:\\n\",\n]\n\n\ndef fmt_infer(b, tpl):\n    qs, ss = b[\"question\"], b[\"solution\"]\n    return {\"text\": [tpl.format(q, str(s)) for q, s in zip(qs, ss)]}\n\nvalid_formatted_list = []\nfor tpl in PROMPTS:\n    valid_formatted_list.append(\n        valid_ds.map(lambda b: fmt_infer(b, tpl), batched=True, remove_columns=valid_ds.column_names)\n    )\n\ntest_formatted_list = []\nfor tpl in PROMPTS:\n    test_formatted_list.append(\n        test_ds.map(lambda b: fmt_infer(b, tpl), batched=True, remove_columns=test_ds.column_names)\n    )\n\nprint(f\"valid: {len(valid_ds)}, test: {len(test_ds)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T23:40:20.397114Z","iopub.execute_input":"2025-11-01T23:40:20.397364Z","iopub.status.idle":"2025-11-01T23:40:50.850689Z","shell.execute_reply.started":"2025-11-01T23:40:20.397345Z","shell.execute_reply":"2025-11-01T23:40:50.849863Z"}},"outputs":[{"name":"stdout","text":"from Hugging Face ..\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e407a1e6b2ee4677a407b2e0f32b6db3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00002.parquet:   0%|          | 0.00/195M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01ecc5d6fecd4258896e2a8114f5569e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00001-of-00002.parquet:   0%|          | 0.00/195M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a10233fb9b04fd89eec97e523f8606b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/3.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f62eb2cd1a9b4fc883dcedf76ab8ff7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86aa5cf57b3a41468af6418163572aa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4a9679dffb5430d86b3dea457644410"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/98474 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96c3e27fd9174cd388e20847dd5db44a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/98474 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"182c3942a9614e7d9ae0d7168d1045a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/98474 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b6cd291028e4b49ad6ed6e1a2da46a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dffa14b0caf48b1b0819e9b016ad77c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d91a87b4ec1c494f969b3d8b4f7036b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c2df4c32bab4c05ae4213f85d554939"}},"metadata":{}},{"name":"stdout","text":"valid: 98474, test: 10000\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\nclass OnlyAllowTF(LogitsProcessor):\n    def __init__(self, step:int): self.step = step\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        if self.step == 0:\n            mask = torch.full_like(scores, float(\"-inf\"))\n            mask[:, allowed_first_tokens] = 0.0\n            scores = scores + mask\n        return scores\n\n\ndef autotune_chunk(texts, start=16, floor=2, ceil=32):\n    import math\n    chunk = min(start, max(floor, len(texts)))\n    print(f\"find optimal CHUNKï¼Œ start={chunk}...\")\n    while chunk >= floor:\n        try:\n            probe = texts[:min(len(texts), chunk)]\n            _inputs = tokenizer(\n                probe, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN_GEN\n            ).to(model.device)\n            with torch.no_grad():\n                _ = model.generate(\n                    **_inputs,\n                    max_new_tokens=2, do_sample=False, temperature=0.0, top_p=1.0,\n                    pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id,\n                    logits_processor=[OnlyAllowTF(step=0)], use_cache=True,\n                )\n            del _inputs, _\n            if torch.cuda.is_available(): torch.cuda.empty_cache()\n            print(f\" CHUNK = {chunk}\")\n            return min(chunk, ceil) \n        except RuntimeError as e:\n            if \"CUDA out of memory\" in str(e):\n                print(f\"    OOM CHUNK = {chunk}\")\n                if torch.cuda.is_available(): torch.cuda.empty_cache()\n                chunk //= 2 \n            else:\n                print(f\"error: {e}\")\n                raise\n    print(f\"  {floor} return {floor}\")\n    return floor\n\n\ndef generate_TF(batch_texts: List[str], chunk: int = 4) -> List[int]:\n    outs = []\n    CHUNK = chunk\n    for i in range(0, len(batch_texts), CHUNK):\n        chunk_texts = batch_texts[i:i+CHUNK]\n        inputs = tokenizer(\n            chunk_texts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=MAX_LEN_GEN,\n        ).to(model.device)\n        with torch.no_grad():\n            gen = model.generate(\n                **inputs,\n                max_new_tokens=2,\n                do_sample=False,\n                temperature=0.0,\n                top_p=1.0,\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id,\n                logits_processor=[OnlyAllowTF(step=0)],\n                use_cache=True,\n            )\n        decoded = tokenizer.batch_decode(gen, skip_special_tokens=True)\n        tails = [full[len(prompt):].strip() if full.startswith(prompt) else full\n                 for full, prompt in zip(decoded, chunk_texts)]\n        \n       \n        outs.extend([1 if t.lower().lstrip().startswith(\"true\") else 0 for t in tails])\n        \n        del inputs, gen, decoded\n        if torch.cuda.is_available(): torch.cuda.empty_cache()\n    return outs\n\n\ndef generate_vote(ds, formatted_list, chunk=4):\n    import numpy as np\n    votes = np.zeros(len(ds), dtype=int)\n    for i, tpl in enumerate(PROMPTS):\n        print(f\"  Prompt {i+1}/{len(PROMPTS)} ('{tpl.splitlines()[0]}...')\")\n        texts = formatted_list[i][\"text\"]\n        preds = generate_TF(texts, chunk=chunk)\n        votes += np.array(preds)\n    \n    final_preds = (votes >= ((len(PROMPTS) + 1)//2)).astype(int)\n    return final_preds\n\nprint(\"vote function\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T23:40:50.851917Z","iopub.execute_input":"2025-11-01T23:40:50.852182Z","iopub.status.idle":"2025-11-01T23:40:50.866344Z","shell.execute_reply.started":"2025-11-01T23:40:50.852162Z","shell.execute_reply":"2025-11-01T23:40:50.865574Z"}},"outputs":[{"name":"stdout","text":"vote function\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\nprobe_texts = valid_formatted_list[0][\"text\"]\nCHUNK_VALID = autotune_chunk(probe_texts, start=16, ceil=32) \nprint(\"===== run test =====\")\n\nCHUNK_VALID\nCHUNK_TEST = CHUNK_VALID\nprint(f\"===== [Autotune] Test CHUNK = {CHUNK_TEST} =====\")\n\n\ntest_preds = generate_vote(test_ds, test_formatted_list, chunk=CHUNK_TEST)\n\n\nprint(\"\\n create submission.csv...\")\nsubmission = pd.DataFrame({\n    \"ID\": range(len(test_preds)),\n    \"is_correct\": test_preds,\n})\n\nout_path = \"submission.csv\"\nsubmission.to_csv(out_path, index=False)\n\nprint(f\"\\n===== finish=====\")\nprint(f\"saved to: {out_path}\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T23:40:50.867158Z","iopub.execute_input":"2025-11-01T23:40:50.867351Z","execution_failed":"2025-11-01T23:41:24.039Z"}},"outputs":[{"name":"stdout","text":"find optimal CHUNKï¼Œ start=16...\n","output_type":"stream"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":" CHUNK = 16\n===== run test =====\n===== [Autotune] Test CHUNK = 16 =====\n  Prompt 1/3 ('Q:...')\n","output_type":"stream"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"}],"execution_count":null}]}
